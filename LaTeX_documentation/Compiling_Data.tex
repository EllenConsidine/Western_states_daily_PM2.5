\subsection{PM2.5 data}%{Compiling Data}

\subsubsection{Processing PM2.5 data}

These are the scripts that process and compile the PM2.5 data:
\begin{enumerate}[nolistsep]
\item \textul{Script1\_Install\_Pkgs.R} >> install packages

%\item \textul{Define\_directories.R} >> (becoming obsolete) clears all variables and defines directories. Needs to be ran between each of the following scripts. (Want to automate this eventually.) When processing a new batch of data, iterate ``processed\_data\_version'' by one letter and create a new subfolder in /home/Processed\_Data/ named PM25\_data\_part\_* where * is the new processed\_data\_version.

\item \textul{Process\_PM25\_data\_step1.R} >> compiles the various PM2.5 data sources into a single data frame. The only eliminations of data are geographic, to remove states that are neither in our study area.% nor adjacent to it.
Update time frame of study if necessary. The output from this script is a csv file and sink .txt for each  PM\textsubscript{2.5} data source as well as a file with all of the PM2.5 data sources merged together (``PM25\_Step1\_part\_*.csv). This script takes about 6 minutes to run on laptop.
	\begin{enumerate}
	\item For DRI data, put in flags for voltage data outside the range 11-17 V. (These thresholds are somewhat arbitrary, but it was noticed that when the voltage was outside this range, the PM\textsubscript{2.5} concentrations were often absurdly high, e.g., greater than 24,000 ug/m3.
	\end{enumerate}

\item \textul{Process\_PM25\_data\_step2.R} >> cleans the data. This script takes about 5 minutes on a laptop. The following is a list of the quality cuts and changes made to the data:
	\begin{enumerate}[nolistsep]
	\item Replace ``UNKNOWN'' datum in EPA data with ``NAD27'' per Colleen's advice.
	\item Remove negative and NA PM\textsubscript{2.5} concentrations. This includes removing all data for a monitor on a given day if any of the hourly observations were negative.
	\item For the hourly data, remove monitor-days that do not have at least 18/24 observations.
	\item For DRI data, remove data with voltage flags (which includes flags that came with the data and flags that were put in because the battery voltage was outside the range 11-17 V.
	\item For DRI data, remove data at or below 0 L/min for flow. Think about whether a minimum value of flow should be set (higher than zero).
	\item June 6, 2014 24-hr average PM\textsubscript{2.5} concentration from monitor ``Smoke NCFS E-BAM \#1'' (Fire\_Cache\_Smoke\_DRI\_Smoke\_NCFS\_E\_BAM\_N1.csv) is 24,203 ug/m3. There's nothing apparent wrong with the hourly data, however, this is the only day of data that made it through the other quality checks from this data file. This suggests that this monitor is suspect, and will be removed.
	\item Remove data points with lat/lon outside this box: (50,-126) to (25,-101). These values are defined in general\_project\_functions.R. %-93) 
	% if the monitor is near a fire, the air temperature could be quite high \item \textbf{To Do} think about making cuts on any unrealistic air temperatures for DRI data
	%seems unnecessary since both are removed \item \textbf{To Do} need to convert missing values that have a -9999 etc to NA value
	%seems unnecessary \item \textbf{To Do} merge "24-HR BLK AVG" and "24 HOUR" data together in Sample Duration variable
	%One site in Maricopa, AZ has some data points with an observation count of 2 and Observation Percent of 200. The data looks fine otherwise, so I'll keep it. %\item \textbf{To Do} figure out why Observation percent has a max value of 200\%
	% this data point was removed with the other quality cuts %\item \textbf{To Do} figure out if max AQI value of 546 is reasonable
	\item \textbf{To Do} Some DRI files looked like they had hour 20:00 data shifted a couple of columns - look into this and fix it.
	%I think I did this in later scripts. %\item \textbf{To Do} Finish filling in Year, month, day information based on date
	\item \textbf{To Do} look over summary() output and plots of every variable and determine if any other cuts are necessary
	\end{enumerate}

\textul{Process\_PM25\_data\_step3.R} >> convert all PM2.5 data to the same datum (NAD83) and project coordinates. Take the reprojected location info and put it into the data frame with the daily PM\textsubscript{2.5} data. This script takes about 3 minutes on a laptop.

\textul{Process\_PM25\_data\_step4.R} >> Take difference between parts d and b to find what locations/dates are only in part d. This script takes about 15 seconds on a laptop. %combine locations and location/date files for parts b and c. (Part a should be disregarded.) \textbf{To Do} write code to take difference between parts bc and d to get a list of just the dates/locations that are new in part d. \textbf{To Do} process part d in this file. Record of the various parts
\begin{enumerate}
\item part a: early version created while writing code. Disregard.
\item part b: first batch of PM2.5 data that was used to extract predictor data, years 2008-2014
\item part c: county centroids, 2008-2014. This work flow has now been moved to the ``Locations\_of\_interest''.
\item part d: second batch of PM2.5 data, adds AQS data for 2015-2018.

\end{enumerate}

%separate part a locations from part b, etc - to be used for extracting predictor variables to points of monitor locations and points of interest for predicting PM2.5

%\item \textul{write\_shp.py} given listing of all PM2.5 monitor locations (in different datums), write each datum into a different shapefile
%\item \textul{reproject\_shp.py} project all shapefiles into US Equal Area Albers (ESRI 102003); save coordinates
%\item \textul{reproject\_shp\_2.py} transform all original shapefiles into NAD83 datum (ESRI 4269); save coordinates 
%\item \textul{merge\_shp.py} merge all the shapefiles into one
%\item \textul{join\_coordinates.py} combine the geographic coordinates and projection coordinates and write to one CSV

%\item \textul{Process\_PM25\_data\_step5.R} >> script to take the reprojected location info and put it into the data frame with the daily PM\textsubscript{2.5} data. %\textul{Merge\_reprojections\_ML\_Input\_File.R}

\item \textul{Process\_PM25\_data\_step5.R} >> composite replicate data - in process. \textbf{To Do} finish colocated version of code to go with aves version of code. Calls these functions:
  \begin{enumerate}
  \item Combine\_true\_replicates\_R\_function.R
  \item fill\_in\_aves\_coloc\_unique\_PC\_POC\_MN\_function.R
    \begin{enumerate}
    \item concatinate\_within\_column\_function.R
    \end{enumerate}
  \item set\_data\_types\_by\_column\_R\_function.R
  \end{enumerate}
%\item \textul{Plot\_ML\_Input\_File.R} >> create plots, maps, and statistical summary - needs to be changed to take input from De-duplicate code instead of file from Create\_ML\_Input\_File.R
%\item \textbf{to be written} >> merge with satelite and other data
\item \textul{Process\_PM25\_data\_step6.R} >> map locations of monitors by data source/year

\end{enumerate}
% {DeDuplicate\_ML\_Input\_File\_v2.R}

\subsubsection{Notes about very high data points}

June 15, 2012 24-hr average PM\textsubscript{2.5} concentration from monitor ``Smoke \#22'' (Fire\_Cache\_Smoke\_DRI\_Smoke\_22.csv) is 5,638 ug/m3 - can't find any reason, so far, to remove this data point, though it's very odd that the concentrations were low single-digits except for two hours which were extremely high (123,000 and 1000 ug/m3).



