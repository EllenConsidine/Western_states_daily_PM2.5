%\subsection{$\textrm{PM}_{2.5}$ data}%{Compiling Data}

\subsubsection{Processing $\textrm{PM}_{2.5}$ data}

Below are the scripts that process and compile the PM\textsubscript{2.5} data. The ``*'' in each of the file names refers to the current ``processed\_data\_version'' (set in general\_project\_functions.R)  since compiling the data is an ongoing process. 

\begin{enumerate}[nolistsep]
\item \textul{Script1\_Install\_Pkgs.R} >> install packages

\item \textul{Process\_PM25\_data\_step1.R} >> compiles the various PM\textsubscript{2.5} data sources into a single data frame. The only eliminations of data are geographic, to remove states that are not in our study area. Update time frame of study if necessary. The output from this script is a csv file and sink .txt for each  PM\textsubscript{2.5} data source as well as a file with all of the PM\textsubscript{2.5} data sources merged together (``PM25\_Step1\_part\_*.csv''). This script takes about 6 minutes to run on a laptop. %This script creates a main data file as well as a csv of data for each PM2.5 data source and a text file describing the data:
\begin{enumerate}[nolistsep]
\item PM25\_Step1\_part\_*.csv (the main data file)
\item PM25\_CARB\_Step1\_part\_*.csv
\item PM25\_CARB\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_EPA\_Step1\_part\_*.csv
\item PM25\_EPA\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_FireCacheDRI\_Step1\_part\_*.csv
\item PM25\_FireCacheDRI\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_IMPRHR2MF88101\_10010\_Step1\_part\_*.csv
\item PM25\_IMPRHR2MF88101\_10010\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_IMPRHR2RCFM88401\_10010\_Step1\_part\_*.csv
\item PM25\_IMPRHR2RCFM88401\_10010\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_IMPRHR3MF88101\_10006\_Step1\_part\_*.csv
\item PM25\_IMPRHR3MF88101\_10006\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_PCAPS\_Step1\_part\_*.csv
\item PM25\_PCAPS\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_UintahBasin\_Step1\_part\_*.csv
\item PM25\_UintahBasin\_Step1\_part\_*\_combining\_sink.txt
\item PM25\_UtahDEQ\_Step1\_part\_*.csv
\item PM25\_UtahDEQ\_Step1\_part\_*\_combining\_sink.txt
\end{enumerate}
\medskip
Notes that are useful when incorporating new data:
\begin{enumerate}
\item Process\_PM25\_data\_step1.R: change n\_data\_sets to higher number if adding new data source
\item For the Federal Land Manager Database (IMPROVE) data: Download data as described in Section \ref{IMPROVE}. Edit ``skip\_n\_files'' in process\_PM25\_parallel\_wrapper\_function.R so that FMLEdata\_Parameter\_MetaData will load the row with the header `DatasetID, Parameter, Code, AQSCode, Units, Description' for each IMPROVE file.
\end{enumerate}

Note about flag added to data:

	\begin{enumerate}
	\item For DRI data, put in flags for voltage data outside the range 11-17 V. (These thresholds are somewhat arbitrary, but it was noticed that when the voltage was outside this range, the PM\textsubscript{2.5} concentrations were often absurdly high, e.g., greater than 24,000 ug/m3.
	\end{enumerate}

\textbf{Data to include in next batch (f) of PM2.5 data:}

\begin{enumerate}
\item 37 CA mobile air monitors - Charles Pearson indicated heâ€™ll email files after they are internally downloaded and approved for release
\item Fire Cache Monitors DRI (USFS) 10 sites - password protection now removed
\item 2015-2018 CARB data
\item Uintah Basin - need to check with Utah contact if there is more data now
\item UDEQ - need to check for new data
\item Any additional state data we can acquire (e.g., NV, MT) - check with Ellen
\end{enumerate}

\item \textul{Process\_PM25\_data\_step2.R} >> cleans the data. This script takes about 5 minutes on a laptop. This script outputs the following files: 
	\begin{enumerate}[nolistsep]
	\item PM25\_Step2\_part\_*.csv (main cleaned data file)
	\item PM25\_Step2\_part\_*\_sink.txt (description and summaries of the data at each step of the quality cutting)
	\item PM25\_Step2\_part\_*\_Locations.csv (list of unique locations from main cleaned data file)
	\item PM25\_Step2\_part\_*\_Locations\_Dates.csv (list of unique locations/dates from main cleaned data file)
	\item Data\_Removed\_in\_PM25\_Step2\_part\_*.csv (list of unique locations from main cleaned data file)
	\end{enumerate}
The following is a list of the quality cuts and changes made to the data:
	\begin{enumerate}[nolistsep]
	\item Replace ``UNKNOWN'' datum in EPA data with ``NAD27'' per Colleen's advice.
	\item Remove negative and NA PM\textsubscript{2.5} concentrations. This includes removing all data for a monitor on a given day if any of the hourly observations were negative.
	\item For the hourly data, remove monitor-days that do not have at least 18/24 observations.
	\item For DRI data, remove data with voltage flags (which includes flags that came with the data and flags that were put in because the battery voltage was outside the range 11-17 V.
	\item For DRI data, remove data at or below 0 L/min for flow. Think about whether a minimum value of flow should be set (higher than zero).
	\item June 6, 2014 24-hr average PM\textsubscript{2.5} concentration from monitor ``Smoke NCFS E-BAM \#1'' (Fire\_Cache\_Smoke\_DRI\_Smoke\_NCFS\_E\_BAM\_N1.csv) is 24,203 ug/m3. There's nothing apparent wrong with the hourly data, however, this is the only day of data that made it through the other quality checks from this data file. This suggests that this monitor is suspect, and will be removed.
	\item Remove data points with lat/lon outside this box: (50,-126) to (25,-101). These values are defined in general\_project\_functions.R. %-93) 
	\item Remove data outside the study period (defined in general\_project\_functions.R).
	\item Remove data with ``Event\_Type'' = ``Excluded''
	\item Remove data with more than 0.001 degrees variation in Lat/lon within a day
	\item Remove data from monitor ``USFS R2-265'' (Fire\_Cache\_Smoke\_USFS\_R2-265.csv) between October 2016 - May 2017. The concentrations (some higher than 65,000,000 ug/m3) and the behavior of the concentrations (frequently changing by exactly 1000 ug/m3 from one hour to the next) are unrealistic. The data outside this time frame look more reasonable.
	\item Remove data from monitor ``USFS R2-264'' (Fire\_Cache\_Smoke\_USFS\_R2-264.csv") between October 2016 - October 2017. The behavior of the concentrations (frequently changing by exactly 1000 ug/m3 from one hour to the next) are unrealistic. The data outside this time frame look more reasonable.
	\item Remove data from monitor ``FWS Smoke \#1'' (Fire\_Cache\_Smoke\_DRI\_FWS\_Smoke\_N1.csv) between February 11-14, 2017. The behavior of the concentrations (frequently changing by exactly 1000 ug/m3 from one hour to the next) are unrealistic. The data outside this time frame look more reasonable.
	\item Remove data from monitor ``Smoke \#22'' (Fire\_Cache\_Smoke\_DRI\_Smoke\_22.csv) on June 15, 2012. The behavior of the concentrations (frequently changing by exactly 1000 ug/m3 from one hour to the next) are unrealistic. The data outside this time frame look more reasonable.

	% if the monitor is near a fire, the air temperature could be quite high \item \textbf{To Do} think about making cuts on any unrealistic air temperatures for DRI data
	%seems unnecessary since both are removed \item \textbf{To Do} need to convert missing values that have a -9999 etc to NA value
	%seems unnecessary \item \textbf{To Do} merge "24-HR BLK AVG" and "24 HOUR" data together in Sample Duration variable
	%One site in Maricopa, AZ has some data points with an observation count of 2 and Observation Percent of 200. The data looks fine otherwise, so I'll keep it. %\item \textbf{To Do} figure out why Observation percent has a max value of 200\%
	% this data point was removed with the other quality cuts %\item \textbf{To Do} figure out if max AQI value of 546 is reasonable
	%can't find 20:00 in the wrong columns - maybe those versions were over-written if we downloaded the files again, not sure. %\item \textbf{To Do} Some DRI files looked like they had hour 20:00 data shifted a couple of columns - look into this and fix it.
	%I think I did this in later scripts. %\item \textbf{To Do} Finish filling in Year, month, day information based on date
	%\item \textbf{To Do} look over summary() output and plots of every variable and determine if any other cuts are necessary
	\end{enumerate}

\item \textul{Process\_PM25\_data\_step3.R} >> convert all PM2.5 data to the same datum (NAD83). Take the converted location info and put it into the data frame with the daily PM\textsubscript{2.5} data. This script also rounds all lat/lon info to 5 digits. This script takes about 3 minutes on a laptop. This script outputs these files:

\begin{enumerate}[nolistsep]
\item PM25\_Step3\_part\_*\_NAD83.csv (main data file)
\item PM25\_Step3\_part\_*\_Locations\_NAD83\_include\_old\_projection.csv
\item PM25\_Step3\_part\_*\_Locations\_NAD83.csv
\item PM25\_Step3\_part\_*\_Locations\_Dates\_NAD83\_include\_old\_projection.csv
\item PM25\_Step3\_part\_*\_Locations\_Dates\_NAD83.csv
\end{enumerate}

%\item \textul{Define\_directories.R} >> (becoming obsolete) clears all variables and defines directories. Needs to be ran between each of the following scripts. (Want to automate this eventually.) When processing a new batch of data, iterate ``processed\_data\_version'' by one letter and create a new subfolder in /home/Processed\_Data/ named PM25\_data\_part\_* where * is the new processed\_data\_version.

\item \textul{Process\_PM25\_data\_step4\_parallel.R} >> composite replicate data and data where there are co-located monitors. This script produces two different versions of the data:   %\textbf{To Do} finish colocated version of code to go with aves version of code.
\begin{enumerate}
\item PM25\_Step4\_part\_*\_de\_duplicated\_aves\_ML\_input.csv takes the averages of all available co-located data
\item PM25\_Step4\_part\_e\_de\_duplicated\_aves\_prioritize\_24hr\_obs\_ML\_input.csv prioritizes data that was originally a 24-hour observation (typically FRM/filter-based measurements) over the data that was an average of hourly observations. 
\end{enumerate}

 Calls these functions:
  \begin{enumerate}
  \item PM25\_station\_deduplicate\_aves\_parallel.fn%Combine\_true\_replicates\_R\_function.R - note that the minimum Observation Percent is kept, not the mean.
  \item PM25\_station\_deduplicate\_aves\_parallel.fn
  \item prioritize\_daily\_obs\_over\_hourly.fn (only called if de\_duplication\_method is set to ``prioritize\_24Hour\_Obs''
  \item fill\_input\_mat\_aves.fn %fill\_in\_aves\_coloc\_unique\_PC\_POC\_MN\_function.R
    \begin{enumerate}
    \item concatinate\_within\_column\_function.R
    \item concatinate\_vector\_of\_strings.fn
    \end{enumerate}

  %\item set\_data\_types\_by\_column\_R\_function.R
  \end{enumerate}
%\item \textul{Plot\_ML\_Input\_File.R} >> create plots, maps, and statistical summary - needs to be changed to take input from De-duplicate code instead of file from Create\_ML\_Input\_File.R
%\item \textbf{to be written} >> merge with satelite and other data
\item \textul{Process\_PM25\_data\_create\_report.R} >> map locations of monitors by data source/year

%separate part a locations from part b, etc - to be used for extracting predictor variables to points of monitor locations and points of interest for predicting PM2.5

%\item \textul{write\_shp.py} given listing of all PM2.5 monitor locations (in different datums), write each datum into a different shapefile
%\item \textul{reproject\_shp.py} project all shapefiles into US Equal Area Albers (ESRI 102003); save coordinates
%\item \textul{reproject\_shp\_2.py} transform all original shapefiles into NAD83 datum (ESRI 4269); save coordinates 
%\item \textul{merge\_shp.py} merge all the shapefiles into one
%\item \textul{join\_coordinates.py} combine the geographic coordinates and projection coordinates and write to one CSV

%\item \textul{Process\_PM25\_data\_step5.R} >> script to take the reprojected location info and put it into the data frame with the daily PM\textsubscript{2.5} data. %\textul{Merge\_reprojections\_ML\_Input\_File.R}

\end{enumerate}
% {DeDuplicate\_ML\_Input\_File\_v2.R}

\subsubsection{Determine which dates/locations are new to most recent ``processed\_data\_version'' }

\textul{Separate\_Locations\_Dates\_by\_processed\_data\_version.R} >> Take difference between parts d and b to find what locations/dates are only in part d. This script takes a few minutes on a laptop. 

\begin{enumerate}[nolistsep]
\item part a: early version created while writing code. Disregard.
\item part b: first batch of PM2.5 data that was used to extract predictor data, years 2008-2014
\item part c: county centroids, 2008-2014. This work flow has now been moved to the ``Locations\_of\_interest''.
\item part d: second batch of PM2.5 data, adds AQS data for 2015-2018.
\item part e: updates IMPROVE and Fire Cache data to include 2008-2018 (whatever portion of that was available for download)
\end{enumerate}

\subsubsection{Notes about very high data points}

All files with daily average concentrations above 1000 ug/m3 were individually inspected. The following Fire Cache monitors have concentrations above 1000 ug/m3 and were kept because the file looked ok (at least there was nothing obviously wrong). (Some files shifted hourly concentrations in increments of 1000 ug/m3 and those were removed as described in Step 2 above.)
\begin{enumerate}[nolistsep]
\item RSF Smoke Monitor 1
\item Smoke \#16
\item Smoke 215
\item Smoke 216
\item Smoke USFS R1-307 (May 4, 2015 concentrations seem suspiciously high, but there isn't an obvious reason to remove the data.)
\item Smoke USFS R2-69
\item Smoke USFS R2-922
\end{enumerate}

