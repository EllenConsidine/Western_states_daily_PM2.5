\subsection{MODIS AOD}

\subsubsection*{Data Source}

\begin{itemize}[nolistsep]
\item \textbf{Contact}
\item \textbf{Citation/Link}
\item \textbf{Data (local)}
\item \textbf{Geographic Extent}
\item \textbf{Temporal Extent}
\item \textbf{Acknowledgment}
\end{itemize}

\subsubsection*{Brief Description}

We will use AOD estimates from the Deep Blue retrieval algorithm for AOD from the MODIS instrument on the NASA Terra and Aqua satellites (MOD04\_L2 and MYD04\_L2) \citep{Sayer2013}. The MODIS product is available twice daily at a 10 km spatial resolution for cloud-free scenes and is available longer than our 2008-2014  study period 
\citep{MODISMOD04L22017,MODISMYD04L22017}.  

AOD products use cloud filtering algorithms that often remove pixels in the center of the smoke plumes because they are assumed to be clouds due to high reflectivity \citep{kondragunta_revisions_2009}. Given that these can be in the middle of smoke plumes, often the locations most heavily impacted by smoke have missing data for a key variable, AOD. In our previous work in summer in California when rain clouds are incredibly rare, we could be confident that missing values not along the coast were not clouds. However, for this larger study region and time period, this will be a bigger challenge. We will attempt to isolate smoke plumes from true clouds using satellite imagery and smoke plume polygons from NOAA's Hazard Mapping System Fire Smoke Product  \citep{NOAAHazMap2017}. We will then estimate missing values within validated smoke plumes, but not within clouds, using radial basis functions as was done in our previous work \citep{Reid2015}. Radial basis functions are exact interpolation functions that will return observed AOD values where they exist but can interpolate higher values than nearby observations in missing locations, which is needed since the missing values were removed due to their high reflectivity \citep{Reid2015}.


\subsubsection*{Notes}

\subsubsection*{File Format} .hdf

\subsubsection*{Data Filtering and Processing}

\subsubsection*{Final Variable(s)}

\subsubsection*{Methods}

\begin{enumerate}
\item \underline{Download the MODIS AOD data sets from both Terra and Aqua sensors:}\\\\
Using the \href{https://search.earthdata.nasa.gov/search?q=MOD04&ok=MOD04}{NASA EarthData online search tool}, search for the 'MOD04' (Terra) data set. Set temporal extent by drawing polygon and set spatial extent by adjusting the appropriate filter on the web interface. Select the collection and proceed to download data. For data download options, specify "Stage for Delivery" through the "FTPPull" distribution option. Specify the email address for orders to be sent to. Orders will be sent to your email with instructions on how to connect to the FTP server and pull the ordered data into your local workspace through the command line. Because the amount of data being requested is large, the orders will come through several separate emails. Repeat this step for the 'MYD04' (Aqua) data set. All of the raw downloaded data from this step will be in .hdf file format.
\item  \underline{Set up file system for data processing:}\\\\ 
Create a directory locally named `collected\_data`. In this directory, make two child directories named "MOD04\_terra" and "MYD04\_aqua". Follow instructions in email to download data through FTP into the appropriate MODIS directory (`MOD04\_terra` or `MYD04\_aqua`) depending on whether the order is from the Terra or Aqua sensor. Create another directory locally named "processed\_data" at the same level as "collected\_data" (this is where the processed data will eventually go). 
\item \underline{Create.csv files from .hdf files by running `modis\_aod\_create\_csv\_file.py`}\\\\
Run script `modis\_aod\_create\_csv\_file.py` (will need to change input and output filepath at top of script in order to match those of your local setup). This script will take all the .hdf files that you have downloaded and store the lat/long and aod value for non-null pixels. A .csv file will be created for each corresponding .hdf file and stored in your `processed\_data' folder.
\item \underline{Add UTC date time information as columns to each csv file}\\\\
Run script `modis\_aod\_add\_utc\_to\_csv.py`
\item \underline{Merge csv files into one}\\\\
Run script `modis\_aod\_merge\_csv\_files.py` to combine all .csv files into one giant .csv file with script that uses multiprocessing (25 processes). Run script again to combine 25 files into 2 (2 process, header=None). Tried to merge the last two files into one giant .csv file, but was too much for Python. This script also excludes the `point` column, which is unnecessary. So, used Postgres to combine final dataset.
\item \underline{Adjust UTC datetime to local datetime}\\\\
Using Postgres query
\item \underline{Create 24 hour averages for EPA station points}

\end{enumerate}

\subsubsection*{Quality Control}

\subsubsection*{Script Names}

\begin{enumerate}
\item modis\_aod\_create\_csv\_file.py
\end{enumerate}

\subsubsection*{Data File Names}

\begin{enumerate}
\item n/a
\end{enumerate}