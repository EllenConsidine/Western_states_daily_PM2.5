# download-earth-observations
Download scripts to facilitate in downloading and processing various data sets needed for this project.

This folder contains the following scripts:

* **Generic_FTP_download.py**: This script downloads all files in a given FTP URL and saves it to a defined AWS S3 bucket location.

Prerequisites:
1) FTP URL
        
Command line arguments:
argv[1] is the the FTP URL including sub-directories (i.e. "ftp.ncdc.noaa.gov/pub/has/HAS011070577/" make sure to omit the "ftp://" from the beginning)
argv[2] is the local file location to write temp output to (data stored here will be deleted) before writing out to S3 cloud.
        
To run in background (on server so that you can terminate SSH session until the script is finished), run `nohup python Generic_FTP_download.py [argv1] [argv2] &` and take not of PID. A log file of nohup.out will be created as well with print statements indicating when the download is done. Check this log file for progress as it will tell you how many total files to download, the current file that is being downloaded, and in the end, how many files have successfully been downloaded and when the script is finished.

Objective:
Quickly and easily download all data sets at an FTP URL to a designated S3 bucket location. Currently, you will need to change the name of the S3 bucket and subdirectory of choice within the script. Also, enter your Secret Access Key and Access Key into the code.
        
Results:
Downloaded data in specified S3 bucket location
        
---       
* **Generic_FTP_download_local.py**: This script downloads all files in a given FTP URL and saves it to a defined local path.
        Prerequisites:
        1) FTP URL
        
        Command line arguments:
        argv[1] is the the FTP URL including sub-directories (i.e. "ftp.ncdc.noaa.gov/pub/has/HAS011070577/" make sure to omit the "ftp://" from the beginning)
        argv[2] is the local file location to write out to.

        Objective:
        Quickly and easily download all data sets at an FTP URL to a designated local file path.
        
        Results:
        Downloaded data in specified local file path
* **MODIS_FTP_download.py**: This script takes in an available data set on the NASA FTP download site and downloads all data from 2008-2014.

        Prerequisites:
        Have the name of the data set and the URL of the FTP site
        i.e. `MOD14A1` and `ladsweb.nascom.nasa.gov`
        
        Command line arguments:
        argv[1] is the name of the data set i.e. 'MOD14A1'
        argv[2] is the local file location to write out to
        
        Objective:
        This script downloads all datasets in the Western U.S. from 2008-2014 of the data set of interest from the NASA FTP 
        site. We specify the spatial range with a regular expression that chooses the correct tiles from the  MODIS Sinusoidal 
        Tile Grid scheme.
        
        Results: .hdf files for the data set of interest

* **NED_bulk_download.py**: This script takes in a csv with NED download URLs and downloads them one by one to a desired location locally.
        
        Prerequisites:
        1) Have a .csv file that lists the download URLs for each NED file that needs to be downloaded for the study area
        (i.e. `NED_bulk_download_file_list.csv` located in this folder is the one we use for this project
        
        Command line arguments:
        argv[1] is the .csv file containing the download URLs for the different NED data sets
        
        Objective:
        This script goes through each row of the .csv file and downloads the necessary zipped NED file (.zip) to desired path,
        then unzips the NED file.
        
        Results: Original .zip file as well as all NED metadata that is unzipped

* **NED_bulk_download_list.csv**: This .csv file is the file that contains the download URL and other metadata for each file that needs to be downloaded for our study area. This .csv file was generated by accessing the NED download page, adding all orders to the cart, and selecting the option to "Export Items to CSV". 
* **modis_aod_create_csv_file.py**: This script converts the raw MODIS AOD .hdf data into .csv format

        Prerequisites:
        1) Download MODIS AOD data from NASA
        2) Use Python 2.7 interpreter to use pyhdf library
        
        Command line arguments:
        None
        
        Objective:
        This script reads in each MODIS AOD .hdf file (that has been previously downloaded via FTP from NASA) and stores the
        latitude, longitude, and best estimate AOD value into a new, corresponding .csv file. Adjust the output_path and
        collected_data path to reflect your local directories.
        
        Results:
        Corresponding .csv files for each original MODIS AOD .hdf file with id, lat, long, and aod as column headers
        for each file.
        
        Note: This script was adapted from Zev Ross's code which was adapted from Phil Moorefield's code.

* **modis_aod_convert_csv_to_shapefile.py**: This script converts the .csv file containing AOD values and lat/long information into shapefiles

        Prerequisites:
        1) Download MODIS AOD data from NASA
        2) Obtain .csv files containing AOD data (by running script `modis_aod_create_csv_file.py`)
        
        Command line arguments:
        None
        
        Objective:
        This script reads in each .csv file (containing AOD values and lat/long data) and converts each row into a point.
        
        Results: corresponding .shp files for each .csv file with id, lat, long, and aod as field variables for each file.
        
        Note: This script was inspired by Zev Ross's code which was adapted from Phil Moorefield's code.

* **modis_aod_merge_shapefiles_given_day.py** This script merges .shp files that occur on the same day into a single .shp file

        Prerequisites:
        1) Download MODIS AOD data from NASA
        2) Obtain .csv files containing AOD data (by running script `modis_aod_create_csv_file.py`)
        3) Convert .csv files to .shp files (by running script `modis_aod_convert_csv_to_shapefile.py`)
        
        Objective:
        This script loops through each .shp file and combines the files that contain data from the same day
        (detailed in file name)
        
        Results: .shp files that contain all combined AOD data for a given day.
        
        Note: This script was inspired by Zev Ross's code which was adapted from Phil Moorefield's code.